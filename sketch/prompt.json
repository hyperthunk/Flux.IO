{
    "role": "you are the principle engineer who is going to build the I/O subsystem: the heart of our non-blocking, stream processing framework",
    "task": "Carefully read the attached architecture documents and examine the attached prototypes we have built to date. Follow the rest of the instructions in this prompt, taking your time and thinking carefully about each phase of implementation and how it impacts on the whole design. Write high quality code at all times.",
    "top-requirement": {
        "type-safety": "we chose F# explicitly for its type system - there should be no use of `obj` anywhere in our code",
        "detailed-explanation": "each stream processor should provide a type safe contract that describes the data it undertands how to operate on - this should never be relegated to `obj`, but rather should be an explicit type of some kind",
        "type-compatibility-and-wiring": "when wiring stream transducers together, we are essentially composing functions (albeit lifted into some monadic excution context and heavily decorated by our framework) - therefore the input and output types of each transducer should be compatible with the next one in the chain",
        "type-wiring-for-event-handers": "this is a complex challenge in a strict type system - where we are trying to support the definition of a more loosely coupled mesh of processing nodes, some mechanism needs to exist for the types which the node supports to be registered. Type errors CANNOT be allowed into the system. Any type checking in intermediate processing stages must be explicit AND must be hidden from the API layer such that node implementors do not have to worry about this"
    },
    "prompt-attachements": {
        "async-parallel-pipeline-framework-design-unified.md": "unified architecture blueprint",
        "architecture-cross-reference.md": "external architecture components we plan to align to over time",
        "pipeline1.txt": "1st example: streaming abstractions using F# async workflows, ValueTask, IValueTaskSource, etc, for zero-allocation scenarios",
        "pipeline2.txt": "2nd example: further exploring API and implementation options for various sub-components/layers"
    },
    "required-outputs": {
        "design": "a complete design overview in markdown format, suitable for publication as github documentation",
        "core-types": "F# types that are common across layers and modules, in a file named Core.fs",
        "pipeline-implementations": "F# implementations of the pipeline layers, stages, and components",
        "other-implementations": "F# implementations of other components that are needed to support the architecture",
        "test-plan": "you are NOT required to write the tests as part of this task - instead please produce a test plan in a markdown file that describes how you would test the components you have implemented"
    },
    "in-scope": {
        "low-cost-zero-copy": "support for low-cost zero-copy data transfer - this should be the default where possible",
        "layering-of-abstractions": "let consumer code choose the level of abstraction it wants to work with",
        "accumulation-patterns": "declarative accumulation patterns for streams - a consumer can keep waiting until it has enough data to satisfy downstream demands (think carefully about what kind of flexible abstractions can be used to declare these dependencies when you are implementing this feature)",
        "planning-aka-pre-compilation": "where it is possible to analyse a workload and pre-optimise for this, we should definitely do so", 
        "stream-composition-architecture": {
            "description": "support for layering of data processing streams",
            "detailed-explanation": "This architecture allows for the composition of multiple processing stages, each potentially implemented as a separate stream. Lower layers may be dealing with non-blocking I/O operations, whilst higher layers may need to create data structures and compose functions to operate on them.",
            "async-note": "a layer that is composing lower-level inputs into a more formal representation (e.g., json tokens/values into a DOM, or into some custom structure), will still need to be doing its work asyncrhonously - we don't know if a stream processing node will ask for a parallel backend or not - our assumption must be that either a node will explicitly call for a parallelism backend (such as Hopac) to explicitly parallelise work in the way that node sees fit (it effectively forms a sub-system; a sub-system node), or we will be using configuration + instrumentation to decide at runtime where to apply parallelism for a given workload.",
            "non-async-note": "it is possible that a non-async backend will be mandated by some (or all) sub-system nodes. In that case, our I/O subsystem will still behave asyncrhonously, however the downstream nodes will consume, process, and pass on the data synchronously. Note that when a node is pushing data to a sink, the I/O subsystem components handling writes will still need to be async / non-blocking, even if all the nodes between reading/consuming and writing/producing are synchronous",
            "non-functional-components": {
                "back-pressure-and-flow-control": "Implement back-pressure and flow control mechanisms to ensure smooth data processing and prevent overwhelming downstream components. Remember this is the I/O subsystem (so we need to be able to handle high-throughput, low-latency data processing), but also remember that the constructs we develop here will need to flow into the parallelism backends and be utilised by them to tell us to slow down or flush, etc etc"
            },
            "explanation-and-examples": {
                "explanation": "We wish to keep zero-copy behaviour, but higher layers need to hand-off data to externally defined code",
                "examples": {
                    "json-parsing": {
                        "base-layer": "raw reading of data from a variety of stream sources into a 0-copy buffer",
                        "parser-layer": {
                            "description": "needs to call Newtonsoft.Json library code to parse/read/produce JSON tokens, JSON path and JSON property/attribute values",
                            "requirement": "support for both streaming and batch processing",
                            "requirement-detail": {
                                "streaming": "leverage Newtonsoft's forward only streaming parser to process JSON tokens/data as they arrive, rather than building the whole DOM in memory before passing it downstream",
                                "batch": "leverage Newtonsoft's code to build a complete in-memory representation of a JSON document or document fragment before passing it downstream - especially needed for processing json schemas",
                                "additional-batch-context": {
                                    "schema-processing-example": {
                                        "description": "JSON schemas are often split up into multiple files or sections, requiring careful handling of references and dependencies between them.",
                                        "requirement-impact": "an intermediate layer might need to read these files and pass the data to another layer that is being used to resolve references",
                                        "additional-context": {
                                            "description": "This may involve maintaining a mapping of schema identifiers to their definitions, as well as tracking which parts of the schema are being used by which documents.",
                                            "requirement-impact": "The system needs to remain non-blocking, therefore we need a blend of stateful and stateless stream processing to achieve this requirement."
                                        }
                                    }
                                }
                            }
                        },
                        "consumer-layer(s)": {
                            "context": "@hyperthunk/rml-pipeline will use our framework for I/O and parallelism",
                            "important-note": "implementation scenario dependent",
                            "example-scenario-presented": "RML processing pipeline",
                            "layer-requirement-impact": "RML template expansion code needs a way to declare its dependencies so an upstream accumulator can be aware of them.",
                            "layer-description-for-example-scenario": {
                                "description": "internally, the RML pipeline sub-system node will hand-off to a parallelism backend such as the Hopac implementation, where RML dependency groups process high volumes of json data in parallel, converging the results (generated RDF triples) into one or more output streams writing to a database, triple store, files, etc.",
                                "layers-and-behaviours": {
                                    "consume-data": "consumes JSON stream tokens and/or data",
                                    "map-current-path-and-context": "maps current path and context to one or more RML groups running in parallel",
                                    "feeds-data": "feeds data to the group(s)",
                                    "where-groups-have-RML-templates-that-require-expansion": [
                                        "accumulates data until a dictionary is available to support template expansion",
                                        "expands template(s) when data is fully available",
                                        "generates RDF triples and writes them to the next stage"
                                    ],
                                    "output-streams": {
                                        "program-build-time": "stream programmer might have explicitly set up an output stream to write RDF data to e.g., neptune or neo4j",
                                        "dynamic-data-driven": "RML supports statically or dynamically defining output targets in your RML mappings - we may therefore find that some output streams are determined at runtime based on the data being processed."
                                    },
                                    "re-sequencing": "generated RDF data is correlated back to the original layer that was provided the JSON input data",
                                    "output-stream-routing": "based on the RML mappings and the data being processed, we now need to push re-sequenced RDF triples to the appropriate output stream to do the actual writing of data (either to the program-build-time output stream(s), or to the dynamically determined output stream(s), depending on the setup/RML/etc)"
                                }
                            }
                        }
                    },
                    "pre-llm-ml-pipeline-execution": {
                        "common-layers": {
                            "training-data-access": "read training data and provide to other layers",
                            "digital-signing-of-layer-outputs": "allows us to track provenance of layer outputs"
                        },
                        "stages": {
                            "stage-1": {
                                "description": "Extract curated mapping triples → tabular | RDF parser (dotNetRDF)"
                            },
                            "stage-2": {
                                "description": "Build lexical inventories | Custom C# |"
                            },
                            "stage-3": {
                                "description": "Compute embeddings | External service (Python) → persisted vectors |"
                            },
                            "stage-4": {
                                "description": "Join features into RecordSet | C# orchestrator |"
                            },
                            "stage-5": {
                                "description": "Train Role Classifiers | ML.NET (OneVersusRest, FastForest / LightGBM) |"
                            },
                            "stage-6": {
                                "description": "Train Multi-Class Term Suggestor | LightGBM multi-class |"
                            },
                            "stage-7": {
                                "description": "Train Ranker | ML.NET LightGBM ranking |"
                            },
                            "stage-8": {
                                "description": "Train Weight Regressor | LightGBM regressor + calibration |"
                            },
                            "stage-9": {
                                "description": "Infer.NET Bayesian Calibration (optional) | Separate project |"
                            },
                            "stage-10": {
                                "description": "Serialize models & feature schema | ML.NET model .zip + JSON manifest |"
                            },
                            "stage-11": {
                                "description": "Scoring service (REST / library) | ASP.NET Core |"
                            }
                        }
                    }
                }
            }
        }
    },
    "out-of-scope": {
        "parallelism-backends": "we will address these in a later iteration",
        "proposed-architecture-enhancements": {
            "out-of-scope-due-to-priority": {
                "uri-based-connector-registry": "we will address this in a later iteration",
                "pattern-level-dsl-layer": "we will address this in a later iteration",
                "stateful-keyed-processing": "we will address this in a later iteration",
                "processing-semantic-guarantees": {
                    "scope-statement": "we will address this in a later iteration",
                    "important-note": "our design should account for us wanting to add this later on"
                }
            },
            "out-of-scope-due-to-complexity": {
                "description": "the following elements are explicitly out of scope",
                "large-complex-work-items": "any enhancements not explicitly in-scope from 'architecture-cross-reference.md', that are sized above L are out of scope"
            }
        }
    },
    "requirements": {
        "architectural-requirements": {
            "modularity": "the system should be modular, allowing for easy replacement and upgrading of components",
            "testability": "the system should be designed to allow for easy testing and validation of components",
            "extensibility": {
                "description": "the system should be designed to allow for easy extension and customization",
                "mechanisms": {
                    "micro-kernel": "where it fits with our architectural and design goals, we should use a micro-kernel + plugin architecture to allow for easy extension"
                }
            }
        },
        "feature-requirements": {
            "async-parallel-pipeline-framework-design-unified": "all the in-scope features defined in the attachment must be addressed",
            "architecture-cross-reference": "any explicitly in-scope features defined in the attachment must be addressed"
        },
        "functional-requirements": {
            "aligned-layers": {
                "zero-copy-by-default": "leverage tools like Span/Memory<T> and Buffer<T> in the lower layers",
                "leverage-io-completion-ports": {
                    "inspiration": "kestral/ASP.NET; java's non-blocking I/O subsystems",
                    "description": "use underlying non-blocking I/O facilities, I/O completion ports on windows, epoll on linux, kqueue on BSD, etc",
                    "implementation": "use appropriate .NET core APIs and patterns to produce efficient, cross-platform code",
                    "avoid-unmanaged-code": "if possible, avoid implementation patterns that rely on unmanaged code"
                },
                "async-behaviour-by-default": "use F# async workflows and ValueTask / IValueTaskSource where appropriate",
                "hide-async-from-higher-layers": {
                    "description": "ensure that higher layers do not need to be aware of the implementation details of async processing in the lower layers",
                    "explanation": "much like ASP.NET hides the complexity of async processing from developers, writing a node should not require you to think about async at all, but rather you should write monadic code (with suitable access to the parts of the execution environment that your node operates in) that describes how you process an event. The way that the runtime composes your node implementation into a tail-call-optimised recursive async function should be hidden from the developer",
                    "additional-note": "it should be possible to interject code into the machinery that wires a monadic node implementation into the framework if necessary, we just do not want that to be the default for node writers!!!",
                    "additional-note-2": "this is a key part of the design: for the synchronous or deterministic-testing flows, we do not want nodes being written that explicitly create async tasks that we perhaps wish to re-write to some other abstraction!",
                    "implementation-notes": [
                        "do not explicitly expose `Async<T>` or related types unless you need to",
                        "if `Async` types need to travel across layers, hide them in our own wrapper types/abstractions - but be careful that we understand the performance implcations of doing this - e.g., if we need to pass `ValueTask<T>` across layers then there is not much more overhead in hiding it in a sate monad and providing access to the node implementation via a controlled API",
                        "there must be clear semantics for what happens when a node implementation tries to access underlying state - will it block, will it return immediatley with some information about the status of the upstream/downstream, will it give error data nicely, is there a timeout version that can be used and how declarative can we make that!?",
                        "use a monadic style to encapsulate data where it is needed without polluting the 'abstraction space' that code in higher layers needs to work with"
                    ]
                },
                "building-data": "establish clear patterns for constructing and manipulating data structures across layers - we need to be able to take lower level zero-copy data and build it up into higher level abstractions, such as JSON token streams, or a full JSON/XML DOM, or some custom data structures defined in a layer we wrote OR that a user wrote"
            },
            "supported-interaction-styles": ["pipes", "message-exchange", "finite-state-machines"],
            "api-design": ["DSLs", "monads for implementation blocks", "async loops", "compasability"]
        },
        "non-functional-requirements": {
            "system-behaviour": {
                "performance": "must support high-throughput, low-latency data processing",
                "scalability": "must be able to scale both horizontally and vertically",
                "efficiency": "efficient use of resources at all times, minimizing overheads",
                "reliability": "must be robust and handle failures gracefully, with minimal impact on the overall system",
                "fault-tolerance": "must be able to continue operating correctly in the event of failures",
                "robustness": "we must manage the lifecycle of async operations and their associated resources properly - do not leak resources"
            }
        }
    },
    "constraints": {
        "default-dependencies": {
            "description": "please assume we have declared the following dependencies and use them by default",
            "external-dependencies": [
                "FSharp.HashCollections (for HashMap and HashSet)",
                "FSharpPlus (for functors, monads, and arrows, and also for lenses to simplify accessing data structures in code)",
                "FSharpx.Collections (for additional collections and data structures if required)",
                "Newtonsoft.Json (for json parsing and serialization)"
            ]
        },
        "coding": {
            "language": "all code must be written in F# unless there is a transformational benefit to coding in C#, in which case a breakout library built in C# may be included in the solution",
            "module-conventions": {
                "description": "you must adhere to the following module structure",
                "namespace": "the base namespace for all modules will be Flux.IO",
                "module-size-and-complexity": "modules should try to follow the single responsibility principle, focusing on a specific task or functionality",
                "file-size-and-complexity": "each .fs file should contain one top level module, with further nested modules defined within it",
                "module-names": "modules should be clearly named to reflect their purpose and functionality",
                "sub-modules": "modules may contain sub-modules, when structuring them in this way aids readability and understanding of the code OR provides a way to manage visibility/access",
                "file-names": "files should be named according to their contents and follow a consistent naming convention",
                "special-cases": {
                    "description": "in these special cases, a separate file and module (+/- sub-modules) MUST be defined to isolate the component",
                    "examples": [
                        "complex data structures (e.g., pools, ring-buffers, concurrent immutable data structures) that have been purpose built for our solution",
                        "wrappers around existing libraries or frameworks to provide a more idiomatic F# interface or provide an API surface better suited to our needs",
                        "any unmanaged code or wrappers around C# library code we need to use or integrate with"
                    ]
                }
            },
            "coding-conventions": {
                "description": "you must adhere fully to these coding standards",
                "standards": {
                    "simplicity": "implementation should strive for simplicity without sacrificing any of the function or other non-functional requirements",
                    "readability": "code should be easy to read and understand, with clear naming conventions and consistent formatting",
                    "functional": "lean heavily into functional code and use FSharpPlus (for functors, monads, and arrows, and lenses) unless there is a huge performance impact in lower level code for doing so",
                    "interface-naming": "internal interfaces should not be prefixed with an 'I' - external code (e.g., a node implementation) should not care whether it is looking at an interface vs a class, since we are not using inheritance as a mechanism for extension by external code (although we may use it internally)",
                    "idiomatic-fsharp": {
                        "description": "code should make use of F# idioms and best practices",
                        "correct-fsharp": "ensure your F# code does not use invalid constructs such as 'return' or 'break'; following the 'functional-programming' conventions will aid with this",
                        "functional-programming": {
                            "immutability-by-default": {
                                "variables": "avoid using mutable variables wherever possible",
                                "members-and-data-structures": "prefer immutable data structures and types except in performance-critical code"
                            },
                            "recursion-not-iteration": "always favor tail-call-optimised recursion over iteration where appropriate - avoid for and while loops whever possible",
                            "pipelining-vs-control-structures": {
                                "favour-composition": "unless on the hot path you need to optimise as much as possible for performance, use function composition and pipelining to build up complex operations",
                                "avoid-side-effects": "strive to avoid side effects in your functions, making them easier to reason about and compose - side-effecting code should be separated from pure code where possible"
                            },
                            "railway-oriented-programming": {
                                "composition-over-inheritance": "favor a compositional style, leveraging functors and monads, instead of building OOP style inheritance hierarchies",
                                "error-handling": "use a railway-oriented approach to error handling, where errors are treated as first-class citizens and can be composed and transformed just like regular values"
                            }
                        }
                    },
                    "csharp-compatibility": {
                        "csharp-wrapper-layer": "we will be providing a C# wrapper layer around the F# code later on, so leave room in your design thinking to accomodate this",
                        "leveraging-csharp": "where leveraging csharp (or csharp libraries) is the best way to achieve our functional or non-functional requirements, this is permitted - ensure we provide an idiomatic way for F# code to interact with the C# code"
                    }
                }
            }
        }        
    },
    "other-notes": {
        "things-IO-nodes-might-do": {
            "execute-sparql-queries": {
                "description": "SPARQL queries for reading/writing RDF data"
            },
            "execute-jolt-transformations": {
                "description": "Jolt transformations for transforming data into a specific format"
            },
            "xslt-transformations": {
                "description": "XSLT transformations for transforming XML data into a specific format"
            },
            "write-to-github-repo": {
                "description": "Push generated code to github, possibly leveraging templates for standardizing artefacts"
            },
            "http-listening-endpoint": {
                "description": "Exposing a REST API endpoint for receiving data from a mapped external system"
            },
            "kafka-listening-endpoint": {
                "description": "Exposing a Kafka endpoint for receiving data from a mapped external system"
            },
            "extractor-endpoint": {
                "description": "Exposing an endpoint for extracting data from a mapped external system (e.g., from a DB using R2RML)"
            },
            "http-outbound-endpoint": {
                "description": "Exposing an endpoint for sending data to a mapped external system"
            },
            "kafka-producer-endpoint": {
                "description": "Exposing a Kafka producer endpoint for sending data to a mapped external system"
            },
            "database-endpoint": {
                "description": "Exposing a database endpoint for writing data to a mapped external datastore (e.g., SQL DB)"
            }
        }
    }
}